{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e505cd",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43bcb860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.1 in c:\\users\\moise niyonkuru\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\moise niyonkuru\\anaconda3\\lib\\site-packages (from torch==2.1.1) (3.6.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\moise niyonkuru\\anaconda3\\lib\\site-packages (from torch==2.1.1) (2022.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\moise niyonkuru\\anaconda3\\lib\\site-packages (from torch==2.1.1) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\moise niyonkuru\\anaconda3\\lib\\site-packages (from torch==2.1.1) (4.1.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\moise niyonkuru\\anaconda3\\lib\\site-packages (from torch==2.1.1) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\moise niyonkuru\\anaconda3\\lib\\site-packages (from torch==2.1.1) (2.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\moise niyonkuru\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.1) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\moise niyonkuru\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.1) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os                       # for working with files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41530314",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f50ca7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     25\u001b[0m dataset_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Dataset for Crop Pest and Disease Detection/CCMT Dataset-Augmented/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 26\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     12\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, filename)\n\u001b[0;32m     13\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Resize the image to a consistent size\u001b[39;00m\n\u001b[0;32m     15\u001b[0m img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize pixel values to [0, 1]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m images\u001b[38;5;241m.\u001b[39mappend(img_array)\n",
      "File \u001b[1;32mc:\\Users\\Moise Niyonkuru\\anaconda3\\lib\\site-packages\\PIL\\Image.py:1980\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   1972\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   1973\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1974\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   1975\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   1976\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   1977\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   1978\u001b[0m         )\n\u001b[1;32m-> 1980\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.JPG', '.jpg', '.jpeg', '.png')):  # Assuming images are in common formats\n",
    "                img_path = os.path.join(root, filename)\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img = img.resize((150, 150))  # Resize the image to a consistent size\n",
    "                img_array = np.array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
    "                images.append(img_array)\n",
    "                \n",
    "                # Extract the class label from the subdirectory name\n",
    "                class_name = os.path.basename(root)\n",
    "                labels.append(class_name)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Example usage\n",
    "dataset_directory = './Dataset for Crop Pest and Disease Detection/CCMT Dataset-Augmented/'\n",
    "images, labels = load_dataset(dataset_directory)\n",
    "\n",
    "# Now 'images' is a NumPy array containing your image data, and 'labels' contains corresponding labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67318322",
   "metadata": {},
   "source": [
    "#### Read images data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afc29eb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './PlantVillage-Dataset//train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m train_dir \u001b[38;5;241m=\u001b[39m data_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/train\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m valid_dir \u001b[38;5;241m=\u001b[39m data_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/valid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m diseases \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m diseases\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './PlantVillage-Dataset//train'"
     ]
    }
   ],
   "source": [
    "data_dir= \"./PlantVillage-Dataset/\"\n",
    "train_dir = data_dir + \"/train\"\n",
    "valid_dir = data_dir + \"/valid\"\n",
    "diseases = os.listdir(train_dir)\n",
    "diseases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de790a",
   "metadata": {},
   "source": [
    "## Predictive Model for Crop Pest and Disease Detection    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "653062a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'diseases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal disease classes are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(\u001b[43mdiseases\u001b[49m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'diseases' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Total disease classes are: {}\".format(len(diseases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9927f329",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'diseases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m plants \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m NumberOfDiseases \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m plant \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdiseases\u001b[49m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m plant\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m___\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m plants:\n\u001b[0;32m      5\u001b[0m         plants\u001b[38;5;241m.\u001b[39mappend(plant\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m___\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'diseases' is not defined"
     ]
    }
   ],
   "source": [
    "plants = []\n",
    "NumberOfDiseases = 0\n",
    "for plant in diseases:\n",
    "    if plant.split('___')[0] not in plants:\n",
    "        plants.append(plant.split('___')[0])\n",
    "    if plant.split('___')[1] != 'healthy':\n",
    "        NumberOfDiseases += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dcb1fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Plants are: \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# unique plants in the dataset\n",
    "print(f\"Unique Plants are: \\n{plants}\")\n",
    "# number of unique plants\n",
    "print(\"Number of plants: {}\".format(len(plants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6616d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of diseases: 0\n"
     ]
    }
   ],
   "source": [
    "# number of unique diseases\n",
    "print(\"Number of diseases: {}\".format(NumberOfDiseases))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51483b7e",
   "metadata": {},
   "source": [
    "So we have images of leaves of 14 plants and while excluding healthy leaves, we have 26 types of images that show a particular disease in a particular plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2a9355d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'diseases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Number of images for each disease\u001b[39;00m\n\u001b[0;32m      2\u001b[0m nums \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m disease \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdiseases\u001b[49m:\n\u001b[0;32m      4\u001b[0m     nums[disease] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(train_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m disease))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'diseases' is not defined"
     ]
    }
   ],
   "source": [
    "# Number of images for each disease\n",
    "nums = {}\n",
    "for disease in diseases:\n",
    "    nums[disease] = len(os.listdir(train_dir + '/' + disease))\n",
    "    \n",
    "# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column\n",
    "\n",
    "img_per_class = pd.DataFrame(nums.values(), index=nums.keys(), columns=[\"no. of images\"])\n",
    "img_per_class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "756d299a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: objects cannot be broadcast to a single shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m index \u001b[38;5;241m=\u001b[39m [n \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m38\u001b[39m)]\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnums\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlants/Diseases\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo of images available\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Moise Niyonkuru\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py:2387\u001b[0m, in \u001b[0;36mbar\u001b[1;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[0;32m   2383\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mbar)\n\u001b[0;32m   2384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbar\u001b[39m(\n\u001b[0;32m   2385\u001b[0m         x, height, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, bottom\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, align\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2386\u001b[0m         data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mbar(\n\u001b[0;32m   2388\u001b[0m         x, height, width\u001b[38;5;241m=\u001b[39mwidth, bottom\u001b[38;5;241m=\u001b[39mbottom, align\u001b[38;5;241m=\u001b[39malign,\n\u001b[0;32m   2389\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Moise Niyonkuru\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:1412\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1414\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1415\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1416\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\Moise Niyonkuru\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:2342\u001b[0m, in \u001b[0;36mAxes.bar\u001b[1;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[0;32m   2339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m yerr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2340\u001b[0m         yerr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_dx(yerr, y0, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_yunits)\n\u001b[1;32m-> 2342\u001b[0m x, height, width, y, linewidth, hatch \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2343\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Make args iterable too.\u001b[39;49;00m\n\u001b[0;32m   2344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;66;03m# Now that units have been converted, set the tick locations.\u001b[39;00m\n\u001b[0;32m   2347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orientation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mbroadcast_arrays\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Moise Niyonkuru\\anaconda3\\lib\\site-packages\\numpy\\lib\\stride_tricks.py:538\u001b[0m, in \u001b[0;36mbroadcast_arrays\u001b[1;34m(subok, *args)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# nditer is not used here to avoid the limit of 32 arrays.\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Otherwise, something like the following one-liner would suffice:\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# return np.nditer(args, flags=['multi_index', 'zerosize_ok'],\u001b[39;00m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m#                  order='C').itviews\u001b[39;00m\n\u001b[0;32m    536\u001b[0m args \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(_m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, subok\u001b[38;5;241m=\u001b[39msubok) \u001b[38;5;28;01mfor\u001b[39;00m _m \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m--> 538\u001b[0m shape \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(array\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m shape \u001b[38;5;28;01mfor\u001b[39;00m array \u001b[38;5;129;01min\u001b[39;00m args):\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;66;03m# Common case where nothing needs to be broadcasted.\u001b[39;00m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mc:\\Users\\Moise Niyonkuru\\anaconda3\\lib\\site-packages\\numpy\\lib\\stride_tricks.py:420\u001b[0m, in \u001b[0;36m_broadcast_shape\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the shape of the arrays that would result from broadcasting the\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03msupplied arrays against each other.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# consistently\u001b[39;00m\n\u001b[1;32m--> 420\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# unfortunately, it cannot handle 32 or more arguments directly\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;28mlen\u001b[39m(args), \u001b[38;5;241m31\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;66;03m# ironically, np.broadcast does not properly handle np.broadcast\u001b[39;00m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;66;03m# objects (it treats them as scalars)\u001b[39;00m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;66;03m# use broadcasting to avoid allocating the full array\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plotting number of images available for each disease\n",
    "index = [n for n in range(38)]\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(index, [n for n in nums.values()], width=0.3)\n",
    "plt.xlabel('Plants/Diseases', fontsize=10)\n",
    "plt.ylabel('No of images available', fontsize=10)\n",
    "plt.xticks(index, diseases, fontsize=5, rotation=90)\n",
    "plt.title('Images per each class of plant disease')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd76503",
   "metadata": {},
   "source": [
    "We can see that the dataset is almost balanced for all classes, so we are good to go forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1e326",
   "metadata": {},
   "source": [
    "#### Images available for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04646954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 images for training\n"
     ]
    }
   ],
   "source": [
    "n_train = 0\n",
    "for value in nums.values():\n",
    "    n_train += value\n",
    "print(f\"There are {n_train} images for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a9e09",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image           \u001b[38;5;66;03m# for checking images\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# import torch.nn.functional as F # for functions for calculating loss\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# import torchvision.transforms as transforms   # for transforming images into tensors \u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_grid   \n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFolder  \u001b[38;5;66;03m# for working with classes and images\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# datasets for validation and training\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch                    # Pytorch module \n",
    "import matplotlib.pyplot as plt # for plotting informations on graph and images using tensors\n",
    "import torch.nn as nn           # for creating  neural networks\n",
    "from torch.utils.data import DataLoader # for dataloaders \n",
    "from PIL import Image          # for checking images\n",
    "# import torch.nn.functional as F # for functions for calculating loss\n",
    "# import torchvision.transforms as transforms   # for transforming images into tensors \n",
    "from torchvision.utils import make_grid   \n",
    "from torchvision.datasets import ImageFolder  # for working with classes and images\n",
    "# datasets for validation and training\n",
    "train = ImageFolder(train_dir, transform=transforms.ToTensor())\n",
    "valid = ImageFolder(valid_dir, transform=transforms.ToTensor()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77703513",
   "metadata": {},
   "source": [
    "#### Image shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7943b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m img, label \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape, label)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "img, label = train[0]\n",
    "print(img.shape, label)\n",
    "# total number of classes in train set\n",
    "len(train.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6197c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50942404",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel :\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m train\u001b[38;5;241m.\u001b[39mclasses[label] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(label) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(image\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m show_image(\u001b[38;5;241m*\u001b[39m\u001b[43mtrain\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# for checking some images from training dataset\n",
    "def show_image(image, label):\n",
    "    print(\"Label :\" + train.classes[label] + \"(\" + str(label) + \")\")\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "\n",
    "\n",
    "show_image(*train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0214cb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m show_image(\u001b[38;5;241m*\u001b[39m\u001b[43mtrain\u001b[49m[\u001b[38;5;241m70000\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "show_image(*train[70000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d678f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b0db9d9c30>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the seed value\n",
    "random_seed = 7\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c03063",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# DataLoaders for training and validation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_dl \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtrain\u001b[49m, batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m valid_dl \u001b[38;5;241m=\u001b[39m DataLoader(valid, batch_size, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# DataLoaders for training and validation\n",
    "train_dl = DataLoader(train, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "valid_dl = DataLoader(valid, batch_size, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06baf0e",
   "metadata": {},
   "source": [
    "- `DataLoader` is a subclass which comes from `torch.utils.data`. It helps in loading large and memory consuming datasets. It takes in `batch_size` which denotes the number of samples contained in each generated batch. \n",
    "\n",
    "- Setting `shuffle=True` shuffles the dataset. It is heplful so that batches between epochs do not look alike. Doing so will eventually make our model more robust.\n",
    "\n",
    "- `num_workers`, denotes the number of processes that generate batches in parallel. If you have more cores in your CPU, you can set it to number of cores in your CPU. Since, Kaggle provides a 2 core CPU, I have set it to 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d460c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to show a batch of training instances\n",
    "def show_batch(data):\n",
    "    for images, labels in data:\n",
    "        fig, ax = plt.subplots(figsize=(30, 30))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images, nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a0cdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Images for first batch of training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m show_batch(\u001b[43mtrain_dl\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dl' is not defined"
     ]
    }
   ],
   "source": [
    "# Images for first batch of training\n",
    "show_batch(train_dl) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e6ac1a",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for moving data into GPU (if available)\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available:\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# for moving data to device (CPU or GPU)\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "# for loading in the device (GPU if available else CPU)\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d61935",
   "metadata": {},
   "source": [
    "Checking the device we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f052676",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_default_device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mget_default_device\u001b[49m()\n\u001b[0;32m      2\u001b[0m device\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_default_device' is not defined"
     ]
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a3301",
   "metadata": {},
   "source": [
    "Wrap up our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a05ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DeviceDataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Moving data into GPU\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_dl \u001b[38;5;241m=\u001b[39m \u001b[43mDeviceDataLoader\u001b[49m(train_dl, device)\n\u001b[0;32m      3\u001b[0m valid_dl \u001b[38;5;241m=\u001b[39m DeviceDataLoader(valid_dl, device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DeviceDataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "# Moving data into GPU\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a393b",
   "metadata": {},
   "source": [
    "## Building the model architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80aabf",
   "metadata": {},
   "source": [
    "*We are going to use **ResNet**, which have been one of the major breakthrough in computer vision since they were introduced in 2015.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540b25d",
   "metadata": {},
   "source": [
    "#### Residual Block code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066feb08",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSimpleResidualBlock\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class SimpleResidualBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        return self.relu2(out) + x # ReLU can be applied before or after adding the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f2ce3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(preds))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# base class for the model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mImageClassificationBase\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m     11\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m batch\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# for calculating the accuracy\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "# base class for the model\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                   # Generate prediction\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        acc = accuracy(out, labels)          # Calculate accuracy\n",
    "        return {\"val_loss\": loss.detach(), \"val_accuracy\": acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x[\"val_loss\"] for x in outputs]\n",
    "        batch_accuracy = [x[\"val_accuracy\"] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()       # Combine loss  \n",
    "        epoch_accuracy = torch.stack(batch_accuracy).mean()\n",
    "        return {\"val_loss\": epoch_loss, \"val_accuracy\": epoch_accuracy} # Combine accuracies\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_accuracy']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd2c76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageClassificationBase' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39mlayers)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# resnet architecture \u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mResNet9\u001b[39;00m(\u001b[43mImageClassificationBase\u001b[49m):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_channels, num_diseases):\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ImageClassificationBase' is not defined"
     ]
    }
   ],
   "source": [
    "# Architecture for training\n",
    "\n",
    "# convolution block with BatchNormalization\n",
    "def ConvBlock(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "             nn.BatchNorm2d(out_channels),\n",
    "             nn.ReLU(inplace=True)]\n",
    "    if pool:\n",
    "        layers.append(nn.MaxPool2d(4))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# resnet architecture \n",
    "class ResNet9(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_diseases):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = ConvBlock(in_channels, 64)\n",
    "        self.conv2 = ConvBlock(64, 128, pool=True) # out_dim : 128 x 64 x 64 \n",
    "        self.res1 = nn.Sequential(ConvBlock(128, 128), ConvBlock(128, 128))\n",
    "        \n",
    "        self.conv3 = ConvBlock(128, 256, pool=True) # out_dim : 256 x 16 x 16\n",
    "        self.conv4 = ConvBlock(256, 512, pool=True) # out_dim : 512 x 4 x 44\n",
    "        self.res2 = nn.Sequential(ConvBlock(512, 512), ConvBlock(512, 512))\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(4),\n",
    "                                       nn.Flatten(),\n",
    "                                       nn.Linear(512, num_diseases))\n",
    "        \n",
    "    def forward(self, xb): # xb is the loaded batch\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0069214",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageClassificationBase' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39mlayers)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# resnet architecture \u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mResNet9\u001b[39;00m(\u001b[43mImageClassificationBase\u001b[49m):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_channels, num_diseases):\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ImageClassificationBase' is not defined"
     ]
    }
   ],
   "source": [
    "# Architecture for training\n",
    "\n",
    "# convolution block with BatchNormalization\n",
    "def ConvBlock(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "             nn.BatchNorm2d(out_channels),\n",
    "             nn.ReLU(inplace=True)]\n",
    "    if pool:\n",
    "        layers.append(nn.MaxPool2d(4))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# resnet architecture \n",
    "class ResNet9(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_diseases):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = ConvBlock(in_channels, 64)\n",
    "        self.conv2 = ConvBlock(64, 128, pool=True) # out_dim : 128 x 64 x 64 \n",
    "        self.res1 = nn.Sequential(ConvBlock(128, 128), ConvBlock(128, 128))\n",
    "        \n",
    "        self.conv3 = ConvBlock(128, 256, pool=True) # out_dim : 256 x 16 x 16\n",
    "        self.conv4 = ConvBlock(256, 512, pool=True) # out_dim : 512 x 4 x 44\n",
    "        self.res2 = nn.Sequential(ConvBlock(512, 512), ConvBlock(512, 512))\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(4),\n",
    "                                       nn.Flatten(),\n",
    "                                       nn.Linear(512, num_diseases))\n",
    "        \n",
    "    def forward(self, xb): # xb is the loaded batch\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12a7cb",
   "metadata": {},
   "source": [
    "Now, we define a model object and transfer it into the device with which we are working..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee720e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# defining the model and moving it to the GPU\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mto_device\u001b[49m(ResNet9(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train\u001b[38;5;241m.\u001b[39mclasses)), device) \n\u001b[0;32m      3\u001b[0m model\n",
      "\u001b[1;31mNameError\u001b[0m: name 'to_device' is not defined"
     ]
    }
   ],
   "source": [
    "# defining the model and moving it to the GPU\n",
    "model = to_device(ResNet9(3, len(train.classes)), device) \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651c2f0",
   "metadata": {},
   "source": [
    "*Getting a nicely formatted summary of our model (like in Keras). Pytorch doesn't support it natively. So, we need to install the `torchsummary` library (discussed earlier)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting summary of the model\n",
    "INPUT_SHAPE = (3, 256, 256)\n",
    "# print(summary(model.cuda(), (INPUT_SHAPE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5169872",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ce8ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# for training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(model, val_loader):\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mvalidation_step(batch) \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_loader]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# for training\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "\n",
    "def fit_OneCycle(epochs, max_lr, model, train_loader, val_loader, weight_decay=0,\n",
    "                grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # scheduler for one cycle learniing rate\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "                \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # recording and updating learning rates\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "            \n",
    "    \n",
    "        # validation\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        \n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713bb79",
   "metadata": {},
   "source": [
    "Let's check our validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42a5ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = [evaluate(model, valid_dl)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54a5aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m grad_clip \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m      4\u001b[0m weight_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m\n\u001b[1;32m----> 5\u001b[0m opt_func \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "max_lr = 0.01\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bdf928",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history += fit_OneCycle(epochs, max_lr, model, train_dl, valid_dl, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=1e-4, \n",
    "                             opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda99754",
   "metadata": {},
   "source": [
    "#### Helper functions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a654cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(history):\n",
    "    accuracies = [x['val_accuracy'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs');\n",
    "\n",
    "def plot_losses(history):\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs')\n",
    "    \n",
    "def plot_lrs(history):\n",
    "    lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
    "    plt.plot(lrs)\n",
    "    plt.xlabel('Batch no.')\n",
    "    plt.ylabel('Learning rate')\n",
    "    plt.title('Learning Rate vs. Batch no.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566df1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea4f56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_accuracies(\u001b[43mhistory\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ec0846",
   "metadata": {},
   "source": [
    "## Validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c52c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_losses(\u001b[43mhistory\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8618e8f",
   "metadata": {},
   "source": [
    "## Learning Rate overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed18303",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_lrs(\u001b[43mhistory\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plot_lrs(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9b5e64",
   "metadata": {},
   "source": [
    "# Testing model on test data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2afda67",
   "metadata": {},
   "source": [
    "**We only have 33 images in test data, so let's check the model on all images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271dd73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageFolder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m test_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../input/new-plant-diseases-dataset/test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mImageFolder\u001b[49m(test_dir, transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ImageFolder' is not defined"
     ]
    }
   ],
   "source": [
    "test_dir = \"../input/new-plant-diseases-dataset/test\"\n",
    "test = ImageFolder(test_dir, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ed8e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(test_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/test\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# since images in test folder are in alphabetical order\u001b[39;00m\n\u001b[0;32m      2\u001b[0m test_images\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "test_images = sorted(os.listdir(test_dir + '/test')) # since images in test folder are in alphabetical order\n",
    "test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d677d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    \"\"\"Converts image to array and return the predicted class\n",
    "        with highest probability\"\"\"\n",
    "    # Convert to a batch of 1\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "    # Pick index with highest probability\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    # Retrieve the class label\n",
    "\n",
    "    return train.classes[preds[0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2741b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# predicting first image\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m img, label \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel:\u001b[39m\u001b[38;5;124m'\u001b[39m, test_images[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, Predicted:\u001b[39m\u001b[38;5;124m'\u001b[39m, predict_image(img, model))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "# predicting first image\n",
    "img, label = test[1]\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "print('Label:', test_images[0], ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e794bd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# getting all predictions (actual label vs predicted)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (img, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtest\u001b[49m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel:\u001b[39m\u001b[38;5;124m'\u001b[39m, test_images[i], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, Predicted:\u001b[39m\u001b[38;5;124m'\u001b[39m, predict_image(img, model))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "# getting all predictions (actual label vs predicted)\n",
    "for i, (img, label) in enumerate(test):\n",
    "    print('Label:', test_images[i], ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db2702e",
   "metadata": {},
   "source": [
    "**We can see that the model predicted all the test images perfectly!!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a5295",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece37d6",
   "metadata": {},
   "source": [
    "**There are several ways to save the model in Pytorch, following are the two most common ways**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf09038",
   "metadata": {},
   "source": [
    "1. **Save/Load `state_dict` (Recommended)**\n",
    "\n",
    "When saving a model for inference, it is only necessary to save the trained models learned parameters. Saving the models `state_dict` with the `torch.save()` function will give you the most flexibility for restoring the model later, which is why it is the recommended method for saving models.\n",
    "\n",
    "A common PyTorch convention is to save models using either a `.pt` or `.pth` file extension.\n",
    "\n",
    "Remember that you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f404d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2e164",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# saving to the kaggle working directory\u001b[39;00m\n\u001b[0;32m      2\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./plant-disease-model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m  \n\u001b[1;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), PATH)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# saving to the kaggle working directory\n",
    "PATH = './plant-disease-model.pth'  \n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901520ce",
   "metadata": {},
   "source": [
    "2. **Save/Load Entire Model**\n",
    "\n",
    "This save/load process uses the most intuitive syntax and involves the least amount of code. Saving a model in this way will save the entire module using Pythons [pickle](https://docs.python.org/3/library/pickle.html) module. The disadvantage of this approach is that the serialized data is bound to the specific classes and the exact directory structure used when the model is saved. The reason for this is because pickle does not save the model class itself. Rather, it saves a path to the file containing the class, which is used during load time. Because of this, your code can break in various ways when used in other projects or after refactors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c401c409",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# saving the entire model to working directory\u001b[39;00m\n\u001b[0;32m      2\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./plant-disease-model-complete.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msave(model, PATH)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# saving the entire model to working directory\n",
    "PATH = './plant-disease-model-complete.pth'\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9708a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a2c5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ce29ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a75c4066",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ae17b",
   "metadata": {},
   "source": [
    "ResNets perform significantly well for image classification when some of the parameters are tweaked and techniques like scheduling learning rate, gradient clipping and weight decay are applied. The model is able to predict every image in test set perfectly without any errors !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5881b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
